from collections import defaultdict
from typing import Any, Callable, Optional, Sequence

import jax
import jax.numpy as jnp
import numpy as np
import optax
from ase import Atoms
from jax.random import PRNGKey

from structgan.model import Critic, Generator
from structgan.utils import *


def make_atomic_number_map(atoms_arr: Sequence[Atoms]):
    """Creates a mapping between atomic numbers and internal type indices.

    Args:
        atoms_arr: The sequence of Atoms objects used to create the map.

    Returns:
        A dictionary mapping the atomic numbers to internal type indices.
    """
    atomtypes = set()
    for atoms in atoms_arr:
        for anum in atoms.get_atomic_numbers():
            atomtypes.add(anum)

    result = {}
    for i, anum in enumerate(sorted(atomtypes)):
        result[anum] = i

    return result


def make_training_dict(
    atoms_arr: Sequence[Atoms], descriptor_method: Callable, anum_map: dict[int, int]
):
    """Generates a dictionary holding possible "real descriptors" indexed by
    the atom types.

    Args:
        atoms_arr: The sequence of Atoms objects to take descriptors from.
        descriptor_method: The method used to generate single descriptors.
        anum_map: Mapping between atomic numbers and internal type indices.

    Returns:
        A dictionary of the list of possible "real descriptors" indexed by
        internal types.
    """
    result = defaultdict(list)

    for atoms in atoms_arr:
        pos = atoms.positions
        cell = atoms.cell[...]
        type = np.array([anum_map[anum] for anum in atoms.get_atomic_numbers()])
        descriptors = descriptor_method(pos, type, pos, type, cell)
        descriptors = descriptors.reshape((descriptors.shape[0], -1))

        for t, desc in zip(type, descriptors):
            result[t].append(desc)

    for t in result:
        result[t] = np.array(result[t])

    return result


def make_real_sampler(
    types: Sequence[int], n_batch: int, training_dict: dict[int, list]
):
    """Creates a sampler method which generates "real descriptors".

    Args:
        types: The internal types of the atoms for which "real descriptors" are
            expected.
        n_batch: The number of batches to generate the descriptors for.
        training_dict: The dictionary of descriptors generated by "make_training_dict".

    Returns:
        A sampler method which uses a PRNG key to generate "real descriptors"
            in the shape of (n_batch, len(types), len(descriptor)).
    """
    expected_shape = (n_batch, len(types), training_dict[0][0].shape[-1])
    type_positions = {}

    for t in types:
        type_positions[t] = np.where(types == t)[0]

    def sampler(rng: PRNGKey):
        result = jnp.zeros(shape=expected_shape)

        for t in type_positions:
            rng, key = jax.random.split(rng)
            indices = type_positions[t]
            result = result.at[:, indices, :].set(
                jax.random.choice(key, training_dict[t], (n_batch, len(indices)))
            )

        return result, rng

    return sampler


def create_critic_step(
    critic: Critic,
    generator: Generator,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_crit: optax.GradientTransformation,
):
    """Creates the critic training step.

    Args:
        critic: The Critic to use.
        generator: The Generator to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_crit: The Optax optimizer object for critic training.

    Returns:
        A method which does one step in the critic training. It uses the
        current critic weights, the current generator weights, the critic
        optimizer state, a batch of "real" descriptors and a PRNG key
        respectively. It returns with the updated critic weights, critic
        optimizer state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(descriptor_method)
    postprocess_batch = jax.vmap(postprocess)

    def critic_step(params_crit, params_gen, opt_state_crit, real_desc, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))
        intermediate = generator.apply(params_gen, fake_latent)
        (
            all_fake_pos,
            all_fake_type,
            fake_pos,
            fake_type,
            cell,
        ) = postprocess_batch(intermediate)

        fake_desc = generate_batch_descriptor(
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell
        )
        fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
        real_desc = jnp.reshape(real_desc, (-1, real_desc.shape[-1]))

        def loss(params):
            fake_preds = critic.apply(params, fake_desc)
            real_preds = critic.apply(params, real_desc)

            return jnp.mean(real_preds - fake_preds)

        grads = jax.grad(loss)(params_crit)
        updates, opt_state_crit = optimizer_crit.update(
            grads, opt_state_crit, params_crit
        )
        params_crit = optax.apply_updates(params_crit, updates)

        return params_crit, opt_state_crit, rng

    return critic_step


def create_gradient_penalty_step(
    critic: Critic,
    generator: Generator,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_crit: optax.GradientTransformation,
):
    """Creates the gradient-penalty step for the critic training.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_crit: The Optax optimizer object for critic training.

    Returns:
        A method which does one step of enforcing the gradient-penalty of the
        critic. It uses the current critic weights, the current generator
        weights, the critic optimizer state, a batch of "real" descriptors and
        a PRNG key respectively. It returns with the updated critic weights,
        critic optimizer state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(descriptor_method)
    postprocess_batch = jax.vmap(postprocess)

    def critic_gp_step(params_crit, params_gen, opt_state_crit, real_desc, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))
        intermediate = generator.apply(params_gen, fake_latent)
        all_fake_pos, all_fake_type, fake_pos, fake_type, cell = postprocess_batch(
            intermediate
        )

        fake_desc = generate_batch_descriptor(
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell
        )

        fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
        real_desc = jnp.reshape(real_desc, (-1, real_desc.shape[-1]))

        rng, key = jax.random.split(rng)
        weights = jax.random.uniform(key, shape=(fake_desc.shape[0], 1))
        mid_descriptors = real_desc * weights + fake_desc * (1 - weights)

        def loss(params):
            def criticize(descriptor):
                return critic.apply(params, descriptor)[0]

            critic_grad = jax.grad(criticize)
            critic_grad_batch = jax.vmap(critic_grad)
            grad_norms = jnp.linalg.norm(critic_grad_batch(mid_descriptors), axis=1)

            return jnp.mean((grad_norms - 1) ** 2)

        grads = jax.grad(loss)(params_crit)
        updates, opt_state_crit = optimizer_crit.update(grads, opt_state_crit)
        params_crit = optax.apply_updates(params_crit, updates)

        return params_crit, opt_state_crit, rng

    return critic_gp_step


def create_generator_step(
    critic: Critic,
    generator: Generator,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_gen: optax.GradientTransformation,
):
    """Creates the generator training step.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_gen: The Optax optimizer object for generator training.

    Returns:
        A method which does one step of the generator training. It uses the
        current critic weights, the current generator weights, the generator
        optimizer state and a PRNG key respectively.
        It returns with the updated generator weights, generator optimizer
        state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(descriptor_method)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def generator_step(params_crit, params_gen, opt_state_gen, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))

        def loss(params):
            intermediate = generator.apply(params, fake_latent)
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell = postprocess_batch(
                intermediate
            )

            fake_desc = generate_batch_descriptor(
                all_fake_pos, all_fake_type, fake_pos, fake_type, cell
            )

            fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
            fake_preds = critic.apply(params_crit, fake_desc)

            return jnp.mean(fake_preds)

        grads = jax.grad(loss)(params_gen)
        updates, opt_state_gen = optimizer_gen.update(grads, opt_state_gen)
        params_gen = optax.apply_updates(params_gen, updates)

        return params_gen, opt_state_gen, rng

    return generator_step


def create_full_training_step(
    critic: Critic,
    generator: Generator,
    descriptor_method: Callable,
    postprocess: Callable,
    types: Sequence[int],
    training_dict: dict[int, list],
    optimizer_crit: optax.GradientTransformation,
    optimizer_gen: optax.GradientTransformation,
    n_batch: int,
    n_crit_per_gen: int = 5,
    n_gp_per_crit: int = 2,
):
    """Creates a full WGAN training step.

    Args:
        critic: The Critic to use.
        generator: The Generator to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        types: The internal types of the atoms for which "real descriptors" are
            expected.
        training_dict: The dictionary of descriptors generated by "make_training_dict".
        optimizer_crit: The Optax optimizer object for critic training.
        optimizer_gen: The Optax optimizer object for generator training.
        n_batch: Number of batches in the training.
        n_crit_per_gen: Number of critic training steps per generator steps. Default: 5
        n_gp_pert_crit: Number of gradient penalty steps per critic steps. Default: 2


    Returns:
        A method which does one full training step. It uses the
        current generator weights, the generator optimizer state,
        critic weights, critic optimizer state and a PRNG key respectively.
        It returns with the updated generator weights, generator optimizer
        state, critic weights, critic optimizer state and the next PRNG key.
    """

    sampler = make_real_sampler(types, n_batch, training_dict)

    train_step_gen = create_generator_step(
        critic=critic,
        generator=generator,
        descriptor_method=descriptor_method,
        postprocess=postprocess,
        n_batch=n_batch,
        optimizer_gen=optimizer_gen,
    )

    train_step_crit = create_critic_step(
        critic=critic,
        generator=generator,
        descriptor_method=descriptor_method,
        postprocess=postprocess,
        n_batch=n_batch,
        optimizer_crit=optimizer_crit,
    )

    train_step_gp = create_gradient_penalty_step(
        critic=critic,
        generator=generator,
        descriptor_method=descriptor_method,
        postprocess=postprocess,
        n_batch=n_batch,
        optimizer_crit=optimizer_crit,
    )

    n_points = len(types)

    def train_step(params_gen, opt_state_gen, params_crit, opt_state_crit, rng):
        params_gen, opt_state_gen, rng = train_step_gen(
            params_crit, params_gen, opt_state_gen, rng
        )
        for _ in range(n_crit_per_gen):
            real_desc, rng = sampler(rng)
            real_desc = real_desc.reshape((n_batch * n_points, -1))
            params_crit, opt_state_crit, rng = train_step_crit(
                params_crit, params_gen, opt_state_crit, real_desc, rng
            )
            for _ in range(n_gp_per_crit):
                real_desc, rng = sampler(rng)
                real_desc = real_desc.reshape((n_batch * n_points, -1))
                params_crit, opt_state_crit, rng = train_step_gp(
                    params_crit, params_gen, opt_state_crit, real_desc, rng
                )

        return params_gen, opt_state_gen, params_crit, opt_state_crit, rng

    return train_step
