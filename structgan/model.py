from functools import partial
from typing import Any, Callable, Optional, Sequence

from flax import linen as nn


class Generator(nn.Module):
    """The generator part of the WGAN architecture using dense layers.

    The model turns the latent variables into (n_points*n_dimensions+n_scalars)
    outputs betweeen 0 and 1, which are then used to create the candidate
    structures during postprocessing.

    Args:
        features: The sequence of hidden layer widths.
        n_points: The number of atoms to predict coordinates for.
        n_dimensions: The dimensionality of the space the atoms are placed in.
        n_latent: Number of latent variables.
        n_scalars: Number of extra scalars to predict for postprocessing.
        activation: The activation function.
    """

    features: Sequence[int]
    n_points: int
    n_dimensions: int
    n_latent: int
    n_scalars: int
    activation: Optional[Callable] = partial(nn.leaky_relu, negative_slope=0.1)

    def setup(self):
        self.layers = [nn.Dense(feat) for feat in self.features]
        self.layers += (nn.Dense(self.n_points * self.n_dimensions + self.n_scalars),)

    def __call__(self, x):
        for i, lyr in enumerate(self.layers):
            x = lyr(x)
            if i != len(self.layers) - 1:
                x = self.activation(x)

        x = nn.sigmoid(x)

        return x


class Critic(nn.Module):
    """The critic part of the WGAN architecture using dense layers.

    This model takes the local descriptors of one atom generated by some
    external descriptor generator method and returns the critic value.

    Args:
        features: The sequence of hidden layer widths.
        activation: The activation function.
    """

    features: Sequence[int]
    activation: Callable = nn.leaky_relu

    def setup(self):
        self.layers = [nn.Dense(feat) for feat in self.features]
        self.layers += (nn.Dense(1),)

    def __call__(self, x):
        for i, lyr in enumerate(self.layers):
            x = lyr(x)
            if i != len(self.layers) - 1:
                x = self.activation(x)

        return x
